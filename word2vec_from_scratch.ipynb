{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tutorial: Creating a CBOW Word2Vec Model with PyTorch**\n",
        "Word embeddings are a fundamental concept in natural language processing (NLP) and machine learning. They represent words as dense vectors in a continuous space, capturing semantic relationships between words. One popular technique for generating word embeddings is the Word2Vec model. In this tutorial, we will create a Continuous Bag of Words (CBOW) Word2Vec model using PyTorch.\n",
        "\n",
        "## **Prerequisites**\n",
        "Before we begin, make sure you have the following prerequisites:\n",
        "\n",
        "Python and PyTorch installed on your machine.\n",
        "Basic knowledge of PyTorch and NLP concepts.\n",
        "## **Understanding CBOW Word2Vec**\n",
        "CBOW (Continuous Bag of Words) is a type of Word2Vec model that aims to predict a target word based on its surrounding context words. It works by training a neural network to predict a target word from the context words in a sliding window. The word embeddings learned during this process capture semantic meanings and relationships between words.\n",
        "\n",
        "## **Steps to Create a CBOW Word2Vec Model**\n",
        "We'll break down the process into the following steps:\n",
        "\n",
        "**1.Data Preprocessing:** Tokenize and preprocess your text data.\n",
        "\n",
        "**2.Build Vocabulary:** Create a vocabulary from your text data.\n",
        "\n",
        "**3.Create CBOW Dataset:** Create a CBOW dataset from the tokenized text.\n",
        "\n",
        "**4.Define CBOW Model:** Define the CBOW model architecture.\n",
        "\n",
        "**5.Training:** Train the CBOW model using the dataset.\n",
        "\n",
        "**6.Save the Model:** Save the trained model for later use.\n",
        "\n",
        "**7.Word Embeddings:** Access and use the word embeddings learned by the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "FDcBmtCZNYw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "# Sample text for demonstration\n",
        "corpus = [\"I enjoy deep learning\",\n",
        "          \"Deep learning is fascinating\",\n",
        "          \"Natural language processing is important\",\n",
        "          \"PyTorch is a popular deep learning framework\",\n",
        "          \"Word embeddings capture semantic meanings\"]\n",
        "\n",
        "# Tokenization and preprocessing\n",
        "nltk.download('punkt')\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "# Tokenize and preprocess the text\n",
        "tokenized_corpus = [word_tokenize(doc.lower().translate(translator)) for doc in corpus]\n",
        "\n",
        "# Build vocabulary\n",
        "# Create a vocabulary and reverse vocabulary to map words to indices and vice versa\n",
        "words = [word for doc in tokenized_corpus for word in doc]\n",
        "word_count = Counter(words)\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(word_count.most_common())}\n",
        "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create CBOW dataset\n",
        "context_window = 2  # Number of surrounding words to consider\n",
        "data = []\n",
        "for doc in tokenized_corpus:\n",
        "    for i, target_word in enumerate(doc):\n",
        "        context = [doc[j] for j in range(i - context_window, i + context_window + 1)\n",
        "                   if 0 <= j < len(doc) and j != i]\n",
        "        data.append((context, target_word))\n",
        "\n",
        "# Define CBOW dataset class\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, data, vocab):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target = self.data[idx]\n",
        "        context_idx = [self.vocab[word] for word in context]\n",
        "        target_idx = self.vocab[target]\n",
        "        return context_idx, target_idx\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 100\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Create CBOW dataset and DataLoader\n",
        "cbow_dataset = CBOWDataset(data, vocab)\n",
        "dataloader = DataLoader(cbow_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Define the CBOW model\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)  # Correct output size\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded_sum = torch.sum(embedded, dim=0)  # Sum along the batch dimension\n",
        "        out = self.linear(embedded_sum)\n",
        "        return out\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = CBOW(vocab_size, embedding_dim)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for context, target in dataloader:\n",
        "        context = context[0]  # Unpack from list\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Convert context and target to PyTorch tensors\n",
        "        context = torch.LongTensor(context)\n",
        "        target_word = reverse_vocab[target[0].item()]  # Convert target to word\n",
        "        target_idx = torch.LongTensor([vocab[target_word]])  # Convert word to index\n",
        "\n",
        "        output = model(context)\n",
        "\n",
        "        # Reshape the output and target tensors to match\n",
        "        output = output.view(1, -1)  # Reshape to (1, vocab_size)\n",
        "        target_idx = target_idx.view(1)  # Reshape to (1,)\n",
        "\n",
        "        loss = criterion(output, target_idx)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'cbow_word2vec.pth')\n",
        "\n",
        "# Use the trained model to get word embeddings\n",
        "word_embeddings = model.embedding.weight.data.numpy()\n",
        "\n",
        "# Print word embeddings\n",
        "for word, idx in vocab.items():\n",
        "    embedding = word_embeddings[idx]\n",
        "    print(f\"Word: {word}, Embedding: {embedding}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h34vplPrLjsG",
        "outputId": "1eb0574c-e2b7-407a-d970-e18cffb39f7e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 3.314073324203491\n",
            "Epoch 2/100, Loss: 2.4836450719833376\n",
            "Epoch 3/100, Loss: 1.9116132354736328\n",
            "Epoch 4/100, Loss: 1.5604651021957396\n",
            "Epoch 5/100, Loss: 1.3671385204792024\n",
            "Epoch 6/100, Loss: 1.255575248003006\n",
            "Epoch 7/100, Loss: 1.1803887385129928\n",
            "Epoch 8/100, Loss: 1.137332437634468\n",
            "Epoch 9/100, Loss: 1.105860486626625\n",
            "Epoch 10/100, Loss: 1.0843769496679305\n",
            "Epoch 11/100, Loss: 1.0659975409507751\n",
            "Epoch 12/100, Loss: 1.0485157623887063\n",
            "Epoch 13/100, Loss: 1.0426976457238197\n",
            "Epoch 14/100, Loss: 1.0229039108753204\n",
            "Epoch 15/100, Loss: 1.0205603456497192\n",
            "Epoch 16/100, Loss: 0.9991889956593514\n",
            "Epoch 17/100, Loss: 0.9965507072210312\n",
            "Epoch 18/100, Loss: 0.9894008563458919\n",
            "Epoch 19/100, Loss: 0.995814810693264\n",
            "Epoch 20/100, Loss: 0.9882562506198883\n",
            "Epoch 21/100, Loss: 0.9765190842747689\n",
            "Epoch 22/100, Loss: 0.9906574709713459\n",
            "Epoch 23/100, Loss: 0.9932756780087948\n",
            "Epoch 24/100, Loss: 0.9699992328882218\n",
            "Epoch 25/100, Loss: 0.9840547174215317\n",
            "Epoch 26/100, Loss: 0.9764780841767788\n",
            "Epoch 27/100, Loss: 0.9661320077627897\n",
            "Epoch 28/100, Loss: 0.9624374604225159\n",
            "Epoch 29/100, Loss: 0.9726786698400974\n",
            "Epoch 30/100, Loss: 0.9793579468876124\n",
            "Epoch 31/100, Loss: 0.9687039113044739\n",
            "Epoch 32/100, Loss: 0.9831665899604559\n",
            "Epoch 33/100, Loss: 0.9561487244069576\n",
            "Epoch 34/100, Loss: 0.9684493003040552\n",
            "Epoch 35/100, Loss: 0.9585587452352047\n",
            "Epoch 36/100, Loss: 0.9738639748096466\n",
            "Epoch 37/100, Loss: 0.9688769380748272\n",
            "Epoch 38/100, Loss: 0.9560139890760183\n",
            "Epoch 39/100, Loss: 0.967731831073761\n",
            "Epoch 40/100, Loss: 0.9599012672901154\n",
            "Epoch 41/100, Loss: 0.945195732563734\n",
            "Epoch 42/100, Loss: 0.9561913355439902\n",
            "Epoch 43/100, Loss: 0.958735349625349\n",
            "Epoch 44/100, Loss: 0.9548302271962166\n",
            "Epoch 45/100, Loss: 0.9558726586401463\n",
            "Epoch 46/100, Loss: 0.9487391128391027\n",
            "Epoch 47/100, Loss: 0.9472543682903052\n",
            "Epoch 48/100, Loss: 0.9651459157839417\n",
            "Epoch 49/100, Loss: 0.9478301457315683\n",
            "Epoch 50/100, Loss: 0.9600318355858326\n",
            "Epoch 51/100, Loss: 0.9528188590705394\n",
            "Epoch 52/100, Loss: 0.9411341264471411\n",
            "Epoch 53/100, Loss: 0.9612734630703926\n",
            "Epoch 54/100, Loss: 0.9519832068309187\n",
            "Epoch 55/100, Loss: 0.9395208984240889\n",
            "Epoch 56/100, Loss: 0.9537706917524338\n",
            "Epoch 57/100, Loss: 0.9539159747213125\n",
            "Epoch 58/100, Loss: 0.9542266300693154\n",
            "Epoch 59/100, Loss: 0.9365087696537375\n",
            "Epoch 60/100, Loss: 0.9504898978024721\n",
            "Epoch 61/100, Loss: 0.9405567305162549\n",
            "Epoch 62/100, Loss: 0.9359571612626314\n",
            "Epoch 63/100, Loss: 0.9427625173330307\n",
            "Epoch 64/100, Loss: 0.9623780603334308\n",
            "Epoch 65/100, Loss: 0.9268465904518962\n",
            "Epoch 66/100, Loss: 0.9474686930701136\n",
            "Epoch 67/100, Loss: 0.9554025188088417\n",
            "Epoch 68/100, Loss: 0.9281311227753758\n",
            "Epoch 69/100, Loss: 0.9400340603291988\n",
            "Epoch 70/100, Loss: 0.9558489327505231\n",
            "Epoch 71/100, Loss: 0.948705911450088\n",
            "Epoch 72/100, Loss: 0.9537637197971344\n",
            "Epoch 73/100, Loss: 0.9434871556982398\n",
            "Epoch 74/100, Loss: 0.9362761131301522\n",
            "Epoch 75/100, Loss: 0.9516422888636589\n",
            "Epoch 76/100, Loss: 0.957248124666512\n",
            "Epoch 77/100, Loss: 0.9632130777463317\n",
            "Epoch 78/100, Loss: 0.9425651327520609\n",
            "Epoch 79/100, Loss: 0.93623590297997\n",
            "Epoch 80/100, Loss: 0.949545890726149\n",
            "Epoch 81/100, Loss: 0.9360151651129126\n",
            "Epoch 82/100, Loss: 0.939190627336502\n",
            "Epoch 83/100, Loss: 0.9424820476770401\n",
            "Epoch 84/100, Loss: 0.9351528892666101\n",
            "Epoch 85/100, Loss: 0.9463876792043447\n",
            "Epoch 86/100, Loss: 0.9535981397330761\n",
            "Epoch 87/100, Loss: 0.9385065944492816\n",
            "Epoch 88/100, Loss: 0.9394984019547701\n",
            "Epoch 89/100, Loss: 0.9359069196879863\n",
            "Epoch 90/100, Loss: 0.9288972852379084\n",
            "Epoch 91/100, Loss: 0.956766444183886\n",
            "Epoch 92/100, Loss: 0.9406643816456198\n",
            "Epoch 93/100, Loss: 0.9396888551115989\n",
            "Epoch 94/100, Loss: 0.9384626787155866\n",
            "Epoch 95/100, Loss: 0.9529561780765653\n",
            "Epoch 96/100, Loss: 0.9410848208516837\n",
            "Epoch 97/100, Loss: 0.9559181135706604\n",
            "Epoch 98/100, Loss: 0.9253036672249436\n",
            "Epoch 99/100, Loss: 0.9448018331453204\n",
            "Epoch 100/100, Loss: 0.9279332327283919\n",
            "Word: deep, Embedding: [-0.37549597 -2.2920005   1.1623038  -1.0574244  -0.5109261   2.1974602\n",
            " -0.17721184 -0.55047303 -0.07532351 -0.9331344   0.07986566 -0.599707\n",
            " -1.0089865  -1.7809463  -0.17338012  0.77936435 -1.6608821  -0.24050987\n",
            "  0.18846114  0.44812313  1.7001722   0.32665735 -1.89446    -0.3983837\n",
            " -0.20127937 -0.8966989   1.240263   -1.1888976  -0.5770069   0.40814424\n",
            "  0.6956398   0.0464367   0.33333728 -0.83574134 -0.1014839   0.00608915\n",
            "  2.1008165  -0.5163236  -0.4473182  -0.24655578 -0.11726408 -1.0530334\n",
            " -0.18555462  0.11604936  1.3127825   1.6006391  -1.1831616  -0.17170885\n",
            " -0.8304203  -0.49153116  1.0030814   1.4250575  -2.3980517  -0.2234891\n",
            "  0.8205735  -0.11448526  1.5252635   0.28864866 -1.2679306  -0.38792026\n",
            "  0.01936814  0.25505337  0.27282882  0.29992285 -0.595732    0.31850204\n",
            " -0.8816552   2.4220982  -0.5607004   2.0722783  -1.3644779  -0.27866784\n",
            " -0.3401416   0.3284597  -0.02442855  0.07935431 -0.86416435 -0.64218605\n",
            " -0.01647037  1.99514    -0.14271617 -0.05406365 -0.6525526   0.3390409\n",
            "  1.1442075   0.32606268 -0.1981048   1.8712915  -0.14562336 -0.27530575\n",
            " -1.7328907  -0.93028384 -0.73672897 -3.0209405  -1.5643215   1.1345122\n",
            " -0.76392305 -0.43102455  0.22488002  0.7557509 ]\n",
            "Word: learning, Embedding: [-1.4118907  -0.5289439  -0.08708773 -0.31862262 -0.43012503  0.21349804\n",
            "  0.330855    0.31777668  0.25300506 -0.35951683 -0.53086865 -2.1658435\n",
            "  0.8526254  -1.6574904  -0.87903684  0.57273835 -0.9635062   0.5511609\n",
            "  0.28139406 -0.3112484  -0.8343544   0.17111799 -0.9242884  -0.6398924\n",
            " -0.9850809   0.737948   -0.38087267  0.5020076   0.3996341   1.7157317\n",
            " -1.6200551   0.6995658  -1.4238424  -0.08850256  0.1947393   0.678452\n",
            " -0.45477888  0.8039848  -0.5971581  -0.2178668  -1.1790663   0.24490209\n",
            " -0.3239966   1.3547441  -1.8846415  -0.69306767  0.7551931   0.28006554\n",
            " -1.366067   -0.24242279  0.87978977  0.23584373 -0.07079225 -0.9382324\n",
            "  0.03332965 -1.0603447  -0.49127007 -1.9983531   0.15016064  0.00429359\n",
            "  0.01872768 -1.161958   -0.16057155 -1.0237257  -0.2786127  -0.39381808\n",
            " -0.1366247  -0.7942021  -2.1493104   1.0992019  -0.49117854 -1.4279422\n",
            " -0.02201137 -1.360898   -0.49392965 -0.85320324 -0.48864937 -0.37628713\n",
            " -1.1640136  -1.6196963  -0.76655287 -0.3125054   1.4334816   0.58788204\n",
            "  1.1620748   0.58518314 -0.60461783 -1.2036518  -0.00705468  0.1224199\n",
            "  0.6299955   0.18963195  1.1259072   1.330611   -0.26456732  1.3940021\n",
            " -0.03249767 -1.2648262  -2.1352863   0.25709412]\n",
            "Word: is, Embedding: [ 8.0918574e-01 -2.6154894e-01  4.9611104e-01  9.7747588e-01\n",
            "  4.0877721e-01  1.0235643e+00 -2.1174736e+00 -1.3052902e-01\n",
            " -3.6046150e-01 -1.9960608e-01  1.2386689e+00  3.9986771e-01\n",
            "  9.1762835e-01  6.5483201e-01 -1.3897102e+00 -1.2602780e+00\n",
            " -6.1640984e-01  3.5220361e-01 -1.8196759e+00 -4.2477340e-01\n",
            "  8.3501953e-01 -1.9565052e+00  6.6527748e-01  1.0053257e+00\n",
            "  7.6403743e-01  1.6787712e-01 -1.1531416e+00 -2.6508464e-02\n",
            " -5.6277865e-01  9.3273938e-01 -8.2831937e-01 -1.1575118e+00\n",
            " -6.5327340e-01 -4.6747038e-01 -1.5604427e+00 -3.3066219e-01\n",
            " -5.7996881e-01 -8.3142355e-02  9.8107122e-02 -8.3681267e-01\n",
            " -1.9638746e+00  7.6974817e-02 -9.9781531e-01  3.8886660e-01\n",
            "  1.9837417e-01  6.8543829e-02 -1.8991771e-01  4.3983182e-01\n",
            " -6.1817086e-01 -7.4979132e-01  1.1970475e+00 -2.8205526e-01\n",
            "  9.4050057e-02 -9.8655462e-01 -2.5491735e-01 -1.9470312e-01\n",
            "  1.5235926e+00  2.6715481e-01 -1.0093292e+00 -1.1510272e+00\n",
            " -3.7298837e-01 -4.3080378e-01  6.7108697e-01 -4.2462721e+00\n",
            " -1.6072700e+00  7.3454803e-01  2.7320969e-01 -1.5575098e+00\n",
            " -7.1018839e-01  2.5557563e+00 -1.3427225e+00  1.1491827e+00\n",
            " -1.9290041e+00  4.0232809e-03 -7.8271818e-01  3.3347914e-01\n",
            "  3.1803796e-01 -6.8005824e-01  1.6241229e-01 -4.3778338e-02\n",
            "  3.1510389e-01  2.8048828e-01 -7.0293832e-01 -7.0725930e-01\n",
            " -4.4308889e-01  3.6683342e-01  4.1469949e-01  8.9596891e-01\n",
            " -2.8116193e-01  2.6354051e+00 -1.4326463e+00 -5.5806775e-02\n",
            " -8.0109495e-01  5.4649782e-01  1.6786048e+00 -2.6488474e-01\n",
            " -1.2982146e+00  6.3470417e-01  1.0708506e+00 -5.9250677e-01]\n",
            "Word: i, Embedding: [-1.6236918   1.0923465   1.0578598   0.16421111 -0.01347588 -0.18073371\n",
            " -0.4537637  -0.00995344  0.24182005 -1.2694515  -0.1724456  -0.6925389\n",
            "  0.00983187  1.2589623  -1.3749517  -0.33490974  1.4410354   1.158452\n",
            " -0.6178541  -0.61221665 -0.03389019 -2.2662342   0.7650094   1.5790036\n",
            "  2.0387588  -0.68684673  0.38410768 -0.28847706  0.8819195  -0.6420936\n",
            " -0.0921048  -2.2362893   3.0778673  -0.8345417  -0.13291477  0.1404081\n",
            "  1.0298041  -1.1918565  -0.10114602 -0.6956421  -0.6454638   0.8084815\n",
            " -0.4173441   0.06296947  2.6079144  -0.46671572  0.31013682  1.0001738\n",
            "  0.27467954  1.7532641  -0.6219593  -0.6563605  -0.4647046   0.871314\n",
            " -0.5702179  -1.6725888   0.6099203   0.32050022 -2.967305   -0.08327972\n",
            " -0.13372603  1.0809935  -1.2130722  -1.1456094  -0.2919269  -0.11596516\n",
            " -0.3034458   0.4767265   0.85062265 -0.06251377  0.48148054  0.4170391\n",
            "  0.740783    0.59347343 -1.9968379  -1.329877    1.5038655   1.7986201\n",
            "  1.43323    -1.1891018  -0.22241363  1.1266047   0.06759882  1.5648496\n",
            "  0.35726538 -0.03248118 -0.98391706  1.4783496  -1.5074761  -1.7545499\n",
            " -0.5573359  -1.2875416   1.2753749  -0.50574535  0.10647169  1.0891098\n",
            " -0.3608062  -0.6508878  -0.24387226  1.2288972 ]\n",
            "Word: enjoy, Embedding: [ 1.2169662   0.855759    0.5901954   0.79580694  0.74940497  1.1207546\n",
            "  2.1145444  -0.9020341  -0.65604234 -0.9460655  -1.9480454   2.7058132\n",
            "  0.9483112   0.3598131  -0.27544257 -0.03754498  0.46637675  0.98431814\n",
            " -0.5933641  -0.4764817  -0.7942323   1.852375    0.6931258  -0.90911275\n",
            "  1.319098    1.2994592   0.32206178  0.7655535  -0.04140229  1.3890351\n",
            "  1.2357603   1.6935548  -0.13014513  1.0406946   1.2523615  -0.34244704\n",
            " -0.12334722 -0.2191266   0.88653487  1.8965355   1.1249722   0.7728924\n",
            " -0.01542245 -0.8301399   0.11794195  1.1043305   0.40604886 -1.1980791\n",
            " -1.8481343  -0.53895897 -0.3439259  -0.8834447  -0.3445861  -0.4793109\n",
            "  0.7383909  -0.2741338   0.63546526 -1.0575095  -0.31941223  0.00955123\n",
            " -0.64332086 -0.15000203  1.2604507  -0.23559357 -0.7858348  -0.07727339\n",
            " -0.10346834 -0.14789446  0.0177445   1.1568035   2.3150582  -0.57797575\n",
            " -1.4319922   0.4069702  -0.85027105 -1.7705886  -1.632321   -1.0287104\n",
            " -0.26579309 -0.04003678 -1.833646   -1.5021     -0.3871729   0.09732358\n",
            " -0.66099036 -1.753133   -1.1078423  -0.1189255   0.2861142  -1.3229005\n",
            " -1.2977625  -0.6747396  -0.07139765 -0.48713264  0.10802053 -0.55003935\n",
            " -0.1510416  -2.0364501  -0.98009115 -0.5876926 ]\n",
            "Word: fascinating, Embedding: [-1.7533467   0.571705    1.7333952   0.53887844 -1.2111092  -0.6156917\n",
            " -1.8858967   0.9224006   0.80091757  1.2486637   0.44317332 -0.31647164\n",
            " -1.5053856   0.20689225  1.4673253  -0.392956   -0.2528719   1.2679005\n",
            "  0.4180067   0.8440946   0.26787752  0.08232936 -0.4437237   0.43393102\n",
            "  1.1987908  -0.28078994  2.3138878   0.36027336  1.4367894  -0.256376\n",
            " -0.8879341  -0.2633569   0.803572   -0.00703852  0.53910255 -1.3441349\n",
            "  0.8781097  -0.514777   -2.562605    1.2646914   0.5494016  -0.08227634\n",
            "  1.2158813  -1.4180387   0.4541873   1.1361367  -2.0359676  -1.2377583\n",
            "  0.20453136  0.04807378 -0.5748239   0.5800931  -0.6258933  -0.43850076\n",
            "  0.5512312   1.5663065   0.7540162   1.3554575  -1.9010453  -0.6303366\n",
            " -2.2298849  -1.7679093  -0.6236723  -1.2230712  -1.7266543   0.9468393\n",
            " -1.5229957   0.3109317  -0.26724112  0.5129935  -0.57637     0.0585015\n",
            " -0.9780927   0.26242015  1.3090925  -0.01314156 -1.2027136  -1.4891692\n",
            "  0.01981013 -0.32124466  0.0126278   0.7085684  -0.8247157  -0.08930866\n",
            "  0.2239425  -0.5380622  -1.502492   -0.7749434   0.9119916  -0.3251305\n",
            " -0.4451489  -0.15213418  0.46190426  0.19072662  1.536459    1.5144715\n",
            "  1.2333794  -0.2597      1.3492614   1.6269382 ]\n",
            "Word: natural, Embedding: [-1.5993518   0.06148442  0.11591037  0.40618458  0.9450426   1.9672565\n",
            "  0.36925724 -0.89910656  0.688503    0.55349755 -1.0980783  -0.14103734\n",
            "  0.43890095 -1.4166286  -0.9883499  -1.5040046  -1.431783    1.5769234\n",
            "  1.6840954  -0.3600332  -0.7319073   0.64221424 -0.67861027  0.1335653\n",
            "  1.7176082  -0.6308856  -0.5041994  -0.6802225  -0.13607161  0.38511217\n",
            "  1.7114892   0.38185555  0.7689406  -0.82996243 -0.33505577 -1.547411\n",
            " -1.0655562  -0.03865449  0.62586117  0.2468487  -0.12291047 -1.7018375\n",
            "  0.6485362   0.85315466 -0.5450066   1.3570148  -0.5293995  -2.1373603\n",
            " -0.33056685 -0.7393408   0.20361587  0.9248876  -1.2518259  -1.7440363\n",
            "  1.5409026  -0.02944225  1.381769    1.0823116  -1.8924487   1.4711567\n",
            " -1.5675969  -2.0741625  -0.61448824  0.07584172  0.29596308 -0.89268357\n",
            " -2.269621    0.5745007   1.1986945   1.4313241  -0.4011447   1.2214478\n",
            " -0.9667631  -0.28137204 -0.26013017  1.3477024   0.54645205  0.33099076\n",
            " -2.5052984   1.3859631  -1.2240338   0.0214584  -0.35673943  0.4008027\n",
            "  2.1597397   0.300017   -0.10967864 -0.5275372  -1.2757996   0.01015583\n",
            "  1.4985145   1.4591508  -1.0482541   0.4710561  -0.90172416 -1.7636408\n",
            "  1.0762343  -0.21586086 -0.52826816 -0.72023565]\n",
            "Word: language, Embedding: [-1.256621   -0.27721232 -1.2201995  -1.7123085   1.4449099  -0.59162724\n",
            "  0.90135294  0.8918051  -1.0611836   0.34947452  1.4440216   0.1852803\n",
            " -0.2569231   0.446106    0.54867685  0.08190521 -0.9775793  -0.0602749\n",
            " -0.22073711 -1.0627629   0.7310398  -0.24198025 -0.4709382  -1.0894148\n",
            "  2.9947486  -0.07189954 -2.0719016  -0.2081747  -0.7116351  -1.3170274\n",
            "  0.42314813 -0.4123782   0.56029683  0.8080027   0.4887741  -0.4591415\n",
            "  1.9219325  -0.48205957  0.25986877  0.7256549   2.7244713   0.09026376\n",
            " -0.56119454  1.4216923  -0.25459048  0.2823221  -0.74527854  1.5268643\n",
            "  0.62084967 -0.18050665  0.2328      1.2209232   0.29691973  0.34975243\n",
            " -0.41777012  0.11197954  0.18459252  1.3615137  -1.861603   -1.8567703\n",
            "  0.3833037   1.8088095  -0.20801215 -0.98766017  0.78642464 -0.74968356\n",
            " -0.12211563  0.09608527 -0.03284824 -1.4304348  -0.1038999   0.57799697\n",
            "  0.70594263  0.4333418  -1.4763362   0.15500903  0.9731851   0.9661458\n",
            "  2.2006416   0.6099497  -0.14898297  0.61965376 -0.6420041  -0.34997314\n",
            "  1.4143641   0.14665261  1.9116747  -0.8183576   1.3485035  -0.33886707\n",
            " -0.13382235  1.0978419   2.4200974  -2.246629   -0.9600091  -2.4802966\n",
            "  0.21842335 -0.5446796  -0.4417686   0.02959412]\n",
            "Word: processing, Embedding: [ 1.33036876e+00 -9.36540425e-01  1.04381597e+00  2.22903395e+00\n",
            " -2.66595781e-01  4.84573364e-01  2.33221352e-01 -1.20992029e+00\n",
            " -1.41945791e+00  1.29434501e-03 -5.67396581e-01 -8.85952711e-01\n",
            "  1.07746817e-01  1.26012921e+00 -7.60258675e-01  1.25013494e+00\n",
            " -1.55642005e-02 -2.16446829e+00  5.93390167e-01  1.32850683e+00\n",
            " -1.91140339e-01  1.38647902e+00  7.17362344e-01  8.64711165e-01\n",
            " -1.42976180e-01  1.27808380e+00 -1.14827597e+00  6.98833883e-01\n",
            "  9.65772688e-01  1.66781342e+00 -7.46857762e-01  6.56988263e-01\n",
            "  1.59156591e-01  1.67517757e+00  5.04244827e-02  7.78959811e-01\n",
            " -9.37236667e-01  1.04820824e+00 -2.31788349e+00  3.14202368e-01\n",
            " -3.56494486e-01  4.97569829e-01 -1.93839058e-01  1.29519784e+00\n",
            "  9.36278403e-01 -1.77108979e+00  7.02518106e-01  1.40152752e+00\n",
            " -5.40996850e-01  2.33926797e+00  1.97439182e+00 -1.37557077e+00\n",
            " -3.56014818e-01 -3.44897926e-01 -1.73287463e+00  2.12158740e-01\n",
            "  6.79611623e-01  9.63241518e-01  2.69603729e-01 -3.76277715e-01\n",
            "  1.43534911e+00 -7.06884205e-01  1.19195282e+00  8.14939439e-01\n",
            " -2.19554365e-01  1.33149099e+00  7.06338584e-01  5.32566428e-01\n",
            "  6.11281991e-01  1.63329220e+00  1.76736641e+00 -1.39915003e-02\n",
            "  3.26464742e-01  8.57745349e-01 -6.44262433e-01 -8.62767696e-01\n",
            "  1.27315164e+00 -1.71931124e+00 -9.74532247e-01 -8.71677101e-01\n",
            " -4.86122757e-01 -1.44943461e-01  1.14195466e+00  2.21662498e+00\n",
            "  1.40121830e+00 -3.31654400e-03 -4.14603382e-01 -1.03475463e+00\n",
            " -3.99380267e-01 -1.60324967e+00 -5.89194000e-01 -1.03586495e+00\n",
            " -1.02355242e+00 -9.50835943e-01 -2.21779630e-01  1.42393574e-01\n",
            " -8.49216878e-02  8.60669374e-01 -1.89966887e-01 -4.22663450e-01]\n",
            "Word: important, Embedding: [ 0.4705446  -0.17748551  0.8116553   1.5031384  -0.45216635 -0.88437545\n",
            " -1.2897725   0.8755591  -0.26431876  0.02583313  0.20030126  0.49436426\n",
            " -0.22018617  0.29982147  1.6351858   1.0689185  -0.6148622   0.6564388\n",
            " -1.0036676   0.10371472  0.77045846  1.5094113  -0.5198761  -1.0781444\n",
            " -1.1892322   0.6320997  -1.2517998   0.02172941 -0.225241   -0.68102336\n",
            " -1.1852877   1.6069393   0.04403738  0.85149807  0.22923444 -0.12694824\n",
            " -1.4456719  -1.5039104   0.30446514  0.24276508 -0.789206    0.92631394\n",
            "  1.6026026  -1.5635498   0.6212496   0.5499151   0.07233154 -0.6682403\n",
            "  0.79660594 -1.2209502  -0.5141057  -0.4707454   0.06227773  0.3114178\n",
            " -0.12092631  0.7514583  -0.9192392  -0.55618185  0.804606    0.17852135\n",
            " -0.57385325  0.43612877  0.4996703   0.52901334 -0.61057717 -0.2179108\n",
            " -1.1719894  -0.95357573  0.40788946 -1.1988997  -0.55128306  0.39476177\n",
            "  0.03331982  0.31467068 -1.9100169   1.1249914  -1.3459991   0.4106936\n",
            " -0.21478559  0.42776543  0.2924987   0.1942383   0.9768632  -1.7573452\n",
            " -0.26973432  1.3063912  -0.85154104 -0.44183993  0.9684652   0.85945344\n",
            "  0.46862456 -3.2240994  -1.549779   -0.2224407  -0.03110247  1.1443955\n",
            "  0.52791905  0.52666724  1.3213907  -2.2344606 ]\n",
            "Word: pytorch, Embedding: [-0.8319482   1.5718259  -0.48782185 -0.5122514   0.8142339  -0.25784215\n",
            "  0.9889994  -0.3540075  -0.05020429  0.4168464  -1.008743   -0.28930902\n",
            " -0.9385662   1.1188626  -0.11929796  1.3444612   0.618456   -0.58687264\n",
            "  0.3355437  -1.3296188  -0.6465945  -0.34726718  0.7424121   2.040179\n",
            "  1.0400703  -1.3887627  -1.129713    2.0023842  -0.78607386 -0.10617797\n",
            "  0.07327539  0.08478919 -0.11553233 -0.2183504  -0.75058347  0.33569646\n",
            " -0.22466344 -2.0075262  -1.5179623  -1.754719   -0.11036592  0.85466343\n",
            " -0.08976766 -0.06975755  1.4915804   2.178989   -0.6235574   0.84397066\n",
            "  0.58944005  2.024594    1.9717488   0.4519877   0.38602293 -0.09226013\n",
            "  1.484641    0.0462036  -1.2422875  -1.3060659   0.4850177   2.9164672\n",
            " -0.77253735 -0.25322038 -0.00682224 -2.2322304  -1.0629566  -0.24735892\n",
            " -0.47428042  0.55955416 -0.2372853  -0.6089754   0.30535182  0.16642068\n",
            "  0.5502597  -0.95182914  0.19909836 -0.593785   -0.4640195  -0.9741601\n",
            " -1.7249774  -0.322493    0.03422314  1.5576514   1.0259361  -0.57120997\n",
            " -0.6135418   1.4097816   0.7233772  -0.3665342  -0.03332127  1.8574438\n",
            " -1.4979358  -0.03487539 -0.8116898   0.23721446 -0.13889831  2.0944643\n",
            "  0.10819222  1.2136909  -1.188577    0.5385885 ]\n",
            "Word: a, Embedding: [-1.4025028e+00  2.7999792e-01  1.9264985e+00  1.2804972e+00\n",
            "  1.0422332e-01  1.5195810e+00 -1.2438469e-01  7.1586740e-01\n",
            "  1.1660913e+00 -1.1069560e+00  2.3261428e-01 -1.3174169e+00\n",
            "  1.7605859e+00  1.8062539e+00 -1.5820671e+00 -1.8194599e+00\n",
            "  6.9937773e-02  1.2714043e-01 -2.2664828e-02  3.7641379e-01\n",
            "  3.6522713e-01  2.9070157e-01 -2.8445092e-01 -8.9684119e-03\n",
            " -3.0712348e-01  1.1162100e+00  5.8689648e-01 -1.4200085e+00\n",
            " -3.6901012e-01 -2.3461044e+00  1.2652433e+00 -5.6068492e-01\n",
            "  3.6783215e-01 -5.5549574e-01 -4.2330348e-01 -5.0244248e-01\n",
            "  1.0085418e+00  2.4724530e-01  1.1830938e+00 -3.9913549e+00\n",
            " -3.5036671e-01  6.5606219e-01 -1.2876793e-02  1.3360888e+00\n",
            " -8.6743480e-01 -3.8253042e-01 -7.3052979e-01  1.3847604e-01\n",
            "  2.2979100e+00  1.6921845e-01 -3.0038437e-01  3.6892769e-01\n",
            "  1.2828361e+00 -6.9102722e-01 -6.7521626e-01 -1.1596117e+00\n",
            " -2.3858806e-01 -3.6994326e-01  9.6063828e-01 -4.9836478e-01\n",
            " -6.0423428e-01  1.2352231e-01 -5.5715883e-01 -4.2371801e-01\n",
            " -4.6113244e-01 -3.2645739e-03 -8.2384974e-01 -6.9807279e-01\n",
            "  2.6711717e+00 -6.3699269e-01 -2.4446595e+00  1.6336627e+00\n",
            "  5.8182192e-01  5.1101285e-01  1.1743183e+00 -8.5325986e-01\n",
            " -1.1090657e-01  4.2485461e-01 -6.8454272e-03 -1.3968431e+00\n",
            " -1.0270311e+00 -2.5305898e+00  7.5379319e-02  6.3387865e-01\n",
            " -3.2317710e-01 -1.1690652e-01 -1.1847848e+00 -1.8651434e+00\n",
            "  8.0593753e-01  1.2555001e+00 -6.8752229e-01 -1.8960251e+00\n",
            " -7.0343912e-01 -6.4144713e-01 -8.9984697e-01  6.5153146e-01\n",
            "  9.7606498e-01 -1.6142577e+00  7.6841229e-01  9.3713723e-02]\n",
            "Word: popular, Embedding: [-1.2972311  -0.7741652   0.87050414  2.0495794  -0.4148586  -0.24637164\n",
            " -0.02031087  0.40427232 -1.4265198   0.5759783  -1.0370909   0.95524096\n",
            "  0.18187492 -0.82745326  0.00840891 -0.17337942 -0.00789752 -0.06131159\n",
            " -1.4107038  -1.318491   -0.05943041 -1.303083   -0.89425236  0.87271315\n",
            " -0.10898096  0.5147713   1.5926937  -2.4991157  -0.87612027 -1.595177\n",
            "  1.5019966   0.6944988   1.974331   -2.0576422   0.7053405   2.129904\n",
            "  0.14730199 -2.66341     0.8558461   0.54977876 -0.38271338 -0.01456144\n",
            " -0.02089962 -1.0270157  -2.1949687  -0.838446    0.29826593  0.5599195\n",
            "  1.0505116  -0.5586025   0.74306625 -0.5707966  -0.77266765 -1.2070044\n",
            " -0.07071093 -0.26980898 -1.4080393   0.2713415  -0.21468744 -0.10233822\n",
            "  1.1683252  -0.12869304  1.2732232  -0.8929178   1.5349404  -1.5467819\n",
            "  1.8961926  -1.0546926  -0.5794281   0.4855567  -0.6226956   0.48227075\n",
            "  0.29945093  0.5919219  -1.5993956   1.4410361  -0.13322705 -1.767652\n",
            " -0.07473743  1.7675765  -0.12782234  1.7668283   0.06472823 -1.3101948\n",
            " -0.8084669  -1.303417   -0.6732237   0.7067557   0.29227614  1.0386156\n",
            " -1.0882226  -0.10045581  0.3661651   0.60016733  0.06638339  0.1878257\n",
            "  0.34628835 -1.3730961  -0.06978662  1.4649209 ]\n",
            "Word: framework, Embedding: [ 6.08690321e-01  1.85483694e-01 -4.27315384e-01  1.39131084e-01\n",
            " -1.82144955e-01  8.38271141e-01 -9.05125022e-01  1.57670581e+00\n",
            " -1.22423165e-01 -2.81249642e-01  4.77419734e-01  1.46751928e+00\n",
            "  1.87799859e+00 -1.18854392e+00  1.04862607e+00  2.67159164e-01\n",
            " -7.79575706e-01  1.23981722e-01 -5.01462877e-01  1.18378472e+00\n",
            " -1.99431121e+00  6.37150526e-01  1.25497246e+00 -1.00595510e+00\n",
            "  1.30149114e+00 -1.30768859e+00  7.73356259e-02  1.11776066e+00\n",
            "  6.29622459e-01 -5.25168478e-01  9.22863662e-01 -9.33276951e-01\n",
            " -7.83327222e-01 -8.14226687e-01 -7.84815669e-01  8.06918219e-02\n",
            " -1.69556454e-01  4.06775355e-01 -4.50298220e-01 -9.26604450e-01\n",
            " -1.00577041e-03 -2.65824914e-01  8.19014251e-01 -1.21268201e+00\n",
            " -7.34531939e-01 -9.94797170e-01  7.06449226e-02 -3.32175463e-01\n",
            " -7.52118587e-01 -1.35561168e+00 -2.87413239e-01  8.11023295e-01\n",
            "  8.86466563e-01  6.94326043e-01  4.26330924e-01  3.31897688e+00\n",
            " -2.23230533e-02  4.81377691e-01 -1.51817789e-02 -1.06415415e+00\n",
            "  9.77615044e-02  6.19838715e-01 -7.04680920e-01 -2.24352494e-01\n",
            "  3.24827075e+00 -1.00202298e+00 -1.83604896e+00  1.93262005e+00\n",
            " -6.50069416e-01  7.60843232e-02  1.20007050e+00 -1.13670588e+00\n",
            " -8.75037313e-01  1.01713157e+00  3.33273292e-01 -5.58055818e-01\n",
            "  1.45226371e+00  1.18632400e+00  1.49158621e+00 -1.91316754e-01\n",
            " -9.73739445e-01  1.18046725e+00 -8.96350265e-01  1.45939755e+00\n",
            "  1.36447382e+00  1.38996589e+00  1.10562965e-01  4.50270683e-01\n",
            " -2.08748713e-01  9.34944928e-01 -4.13409472e-01  6.16104424e-01\n",
            " -1.85416386e-01 -5.90645730e-01  4.66555297e-01  6.27332330e-01\n",
            "  8.25591743e-01  7.81620145e-01  1.22712815e+00 -1.35650003e+00]\n",
            "Word: word, Embedding: [-1.0781791   0.6562083  -0.6989201  -1.2155992  -0.25524676  0.8071197\n",
            " -0.20236725 -1.0382756  -1.6969811  -1.0240269  -1.4811814   0.6935419\n",
            "  0.20720544  1.2286592   0.2642139  -1.0631752  -0.5108864   0.31043637\n",
            "  0.8731514   0.8880316  -1.2230403  -0.17261042 -0.49258685  0.32771492\n",
            "  2.2229857  -0.7897241  -0.02329868  0.01173683  1.2120343   0.4174098\n",
            " -1.5841216   1.022413   -2.1337945  -1.5337511   0.42651123 -1.0422088\n",
            "  1.439173    0.12972817 -0.00760134 -1.8523574   0.24295528  0.12643361\n",
            " -0.27022138  1.3461256   0.12314399 -0.555616   -1.5708028  -1.094969\n",
            " -0.2506253  -0.9196495   1.2507309   0.5011704  -0.3630553  -0.604203\n",
            " -0.01458381  0.02341668 -0.2165551   1.0333135   0.3354117   1.645314\n",
            " -0.26636633  0.1041657   0.33029917 -1.3128074   0.05058474 -0.89550066\n",
            " -0.8505626  -0.17263521 -0.88786674 -0.72032    -0.16788161 -0.760998\n",
            " -0.1568396   0.0036709   0.3058999  -0.7049786   1.0747838   0.17667295\n",
            "  1.277812   -1.2481776  -0.8945302  -0.9349754   0.2120444  -1.4924293\n",
            "  0.23988633 -0.44385147 -0.67189837  0.3614454  -0.607834   -0.84621936\n",
            "  0.13530923  1.5755957  -0.24207444  0.0905929  -1.0401239  -1.03413\n",
            " -0.71899295 -0.77337056 -2.763357    2.0437076 ]\n",
            "Word: embeddings, Embedding: [-0.32003564  1.5425235  -0.2726863   1.5268891   0.31297228  0.8737195\n",
            " -1.0584934  -0.58808947 -0.7593846  -0.46682414 -0.6107896  -1.2716275\n",
            "  0.80861336  0.7368132   0.10944381  0.24094383  0.06571764 -1.2658771\n",
            " -0.05649573 -0.43510276 -0.45374036  0.21977067 -0.726186    1.1995277\n",
            "  2.0934143   1.2829311   0.95180017 -1.2922109  -2.2653368  -1.3343391\n",
            " -1.1935714  -2.2380211   2.0890331  -0.46082172  0.2265754  -0.55398124\n",
            "  1.7514532   0.8064209   1.426009    0.09675869 -0.6976601   0.59219223\n",
            " -0.14512141 -0.6859392  -0.7817817  -1.7718662  -0.6942079  -0.00922606\n",
            " -0.671134    0.22202002 -0.310727    0.0051996   0.35530105 -0.34756216\n",
            " -0.30843267 -2.1702056  -0.30266687 -1.6444534  -0.6894694  -0.09525568\n",
            " -1.1121447  -0.11792658 -0.45032945 -0.5448299  -0.3185247   0.08964634\n",
            " -1.1901398   0.32814524 -1.4423841  -0.4333043  -2.0147853   0.5943548\n",
            "  0.4793682   1.0840133  -0.5262593  -0.22893    -0.23588799  0.9389923\n",
            " -0.2801211   2.2938116   0.05211478 -1.9589723  -1.3933738  -0.87861836\n",
            " -0.10336148  1.527138   -0.8510411   0.3314278  -1.550944   -0.25065053\n",
            "  0.60325867 -0.28836697 -1.8170571   0.05280042  0.20813572 -0.19324495\n",
            " -0.9523496   0.316372   -1.6982626   0.03786706]\n",
            "Word: capture, Embedding: [ 0.29471686  0.5076076   1.1783308   0.5106088  -0.3945928   0.8320768\n",
            " -0.1606278   1.0179323  -0.02210498 -0.48029298 -0.22643057 -0.82873774\n",
            " -0.55256087 -0.7629953   0.65395534  0.17610203  0.09409135  1.1999247\n",
            " -2.007979   -0.04110108 -1.6603875   1.6509614   0.8464062  -0.64940256\n",
            "  0.25566387  0.92993337 -1.1950648   2.4407601   0.62915915  0.2785895\n",
            "  0.93112606 -1.6569916   0.17763783  0.82823426  0.89301497 -1.0694311\n",
            " -0.992262    0.984135    0.07872677  1.4435179   0.04885457  0.7778004\n",
            "  1.4263839   0.96980196 -0.9195723   1.093439   -0.31685898  0.957605\n",
            "  0.38001508 -1.3716092   0.67681974  0.7116928   0.6110228  -0.01571692\n",
            "  0.14274934  1.2033422   1.2988596   1.8688171  -0.17745689 -0.7551443\n",
            " -0.22456618 -1.9849412  -1.0762966   0.85181075 -0.61560434 -0.6425656\n",
            "  1.30029     1.1956127  -0.45191556 -1.2897295  -0.87427676  0.8042047\n",
            " -1.953872    0.35977715  0.6036557   0.52177095 -0.9529844   0.09751616\n",
            "  0.50060606 -0.64693195 -0.5588026  -0.22044569  2.1316836   0.55812234\n",
            "  0.14948274  2.148102   -0.13044034 -0.8466462  -1.122151   -0.26406938\n",
            " -1.3881109  -0.47570238  0.29162908  0.1037368   0.36809695  1.3250476\n",
            "  0.21941723 -0.40080696  0.47185317  0.55599093]\n",
            "Word: semantic, Embedding: [-0.10423458  1.4113331   0.33256695  0.40353197 -0.36284482  0.464803\n",
            "  1.3183217   0.29682162 -0.9441819  -0.706838    0.5686544   0.20956458\n",
            "  1.4365971  -0.31230313 -0.32484946  1.9000258  -1.3791252   0.1435525\n",
            "  0.21659465 -0.27083316 -2.10882    -0.92638826 -0.8951655   0.980671\n",
            "  2.76434    -3.3577154  -0.15144832  0.12836947  1.3853724  -0.02735489\n",
            "  0.6986498   0.94984084 -0.11349703  0.40249255 -1.0090656   1.5035669\n",
            " -0.24167441  1.0707363   0.80360293  0.31259003  0.33875886  0.25222683\n",
            "  0.44909364 -1.557612   -2.1632483  -2.2775197   0.47184908  0.8286392\n",
            "  1.1108105  -0.08863366  0.9010558  -0.616482    2.1215858  -0.5758855\n",
            "  1.340051    0.04237848  1.6130985  -0.0934991   1.0757213   0.47541404\n",
            "  1.993544    1.122351    0.4838528  -1.4850127  -0.79506445  0.4707416\n",
            " -0.22175674 -0.22231703  0.6498044  -0.49010447  0.2532399  -1.4921367\n",
            " -0.34016207 -0.6204526   1.758925    1.0481067   0.46983767  0.9262777\n",
            " -0.5283778  -0.25488502 -1.5111933  -0.7596131   0.29222214  0.39676818\n",
            " -0.46521676 -0.1689688   1.6653492  -0.46194246 -1.1720473   1.13614\n",
            " -0.36275262  0.6579837  -2.2849936  -1.0976207   0.19542147 -1.530822\n",
            "  0.21508746  0.13108444  1.516987    0.10967472]\n",
            "Word: meanings, Embedding: [ 6.5694153e-01 -1.9071602e+00 -2.0126592e-01 -1.8170826e-01\n",
            "  1.3728736e-01 -6.3519537e-01 -6.5782827e-01  3.4613630e-01\n",
            "  4.6774715e-01 -1.0023508e+00  7.7980794e-02 -7.0327878e-01\n",
            " -1.2781746e+00 -1.5036310e+00  8.1829071e-01  5.5971605e-01\n",
            " -1.6992342e-01 -6.7957491e-02  3.6946133e-01  4.2867106e-01\n",
            " -4.8803127e-01 -9.0041381e-01  6.7844015e-01 -3.4966108e-01\n",
            " -6.5749377e-01 -7.1689349e-01  1.6886207e+00 -1.8342268e-01\n",
            "  4.9544924e-01  3.8043964e-01  5.1008631e-02  1.0029317e+00\n",
            " -8.7761497e-01 -8.5645653e-02  4.0594336e-01 -7.7219057e-01\n",
            " -5.2274269e-01 -9.0081912e-01 -1.3505540e+00  1.7857192e+00\n",
            "  7.6156300e-01  5.1670462e-01  2.0784396e-01 -1.2027571e+00\n",
            " -3.5794747e-01 -1.4387529e-03 -6.1916572e-01 -3.8436040e-01\n",
            "  2.3776326e-02  1.8778892e+00  2.6548985e-01 -8.9478868e-01\n",
            "  1.2668375e+00  1.3899138e+00  8.4763741e-01  1.3748363e+00\n",
            "  2.2251812e-01 -4.6543097e-01  9.3616939e-01 -2.5163281e-01\n",
            "  8.0084783e-01  1.0135994e+00  7.1714872e-01  3.7868729e-01\n",
            "  6.1188924e-01 -1.0506669e+00  1.4043101e+00  3.8260311e-01\n",
            "  1.2693119e-01  4.5367995e-01  1.1045681e+00 -5.6612158e-01\n",
            "  5.9150219e-01  6.2422976e-02  1.4696990e-01 -4.9285936e-01\n",
            " -5.1329845e-01 -5.6046063e-01  2.5298968e-01 -4.3509349e-01\n",
            "  2.5850859e-01  1.4408456e-01  1.5975916e-01  3.8542834e-01\n",
            " -5.1628792e-01 -1.5727675e+00  1.3027416e-01 -8.2494986e-01\n",
            "  1.4315284e+00 -6.9660738e-02 -1.9042463e+00  1.3442965e+00\n",
            "  8.4758377e-01  5.4668409e-01 -2.2020481e+00  1.5199645e-01\n",
            " -1.3425041e+00 -2.6507536e-01 -1.2181300e+00  1.5649533e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Explanation of the Code**\n",
        "**Data Preprocessing:** We start by tokenizing and preprocessing the input text data using the NLTK library. This ensures that the text is cleaned and split into individual words.\n",
        "\n",
        "**Build Vocabulary:** We build a vocabulary from the preprocessed text data. The vocabulary consists of unique words, each assigned a unique index.\n",
        "\n",
        "**Create CBOW Dataset:** We create a CBOW dataset by sliding a context window over the tokenized text and pairing the surrounding words with the target word.\n",
        "\n",
        "**Define CBOW Model:** We define the CBOW model using PyTorch. The model consists of an embedding layer and a linear layer.\n",
        "\n",
        "**Training:** We train the CBOW model using the created dataset and standard deep learning training techniques.\n",
        "\n",
        "**Save the Model:** We save the trained model for later use so that you can access the learned word embeddings.\n",
        "\n",
        "**Word Embeddings:** We access and print the word embeddings learned by the model. These embeddings represent words as dense vectors in a continuous space."
      ],
      "metadata": {
        "id": "eeUlnQlYP3Tj"
      }
    }
  ]
}